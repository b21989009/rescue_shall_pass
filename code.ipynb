{"cells":[{"cell_type":"markdown","source":["\n","## Rescue Shall Pass: Emergency Vehicle Detection\n","\n","<br>\n","\n","## Transfer Learning - FasterRCNN\n","\n","<br>\n","\n","## BBM416 Computer Vision. Spring 2023.\n","\n","<br>\n","\n","#### Contributors: Hasim Zafer Cicek, Mehmet Giray Nacakci, Enes Yavuz"],"metadata":{"collapsed":false,"id":"TTY6cacnJz1g"},"id":"TTY6cacnJz1g"},{"cell_type":"markdown","source":["# INITIALIZATION"],"metadata":{"collapsed":false,"id":"l3IeICz5Jz1i"},"id":"l3IeICz5Jz1i"},{"cell_type":"markdown","source":["### Choose Local or Google Collab"],"metadata":{"collapsed":false,"id":"r9uKMju-Jz1j"},"id":"r9uKMju-Jz1j"},{"cell_type":"code","execution_count":1,"outputs":[],"source":["# environment_ = \"local\"\n","environment_ = \"collab\"\n"],"metadata":{"id":"qOFWIHn8Jz1j","ExecuteTime":{"end_time":"2023-06-17T14:39:58.185046Z","start_time":"2023-06-17T14:39:58.179695Z"},"executionInfo":{"status":"ok","timestamp":1687167179181,"user_tz":-180,"elapsed":3,"user":{"displayName":"M Giray Nacakcı","userId":"07330273579537138423"}}},"id":"qOFWIHn8Jz1j"},{"cell_type":"code","execution_count":null,"outputs":[],"source":["root_folder = \"\"\n","\n","\"\"\" Google Collab Online GPU Training \"\"\"\n","# We mainly trained on Google Collab with GPU, which sped up the training around 5 times\n","if environment_ == \"collab\":\n","    from google.colab import drive\n","    drive.mount(\"/content/gdrive\")\n","\n","    root_folder = \"/content/gdrive/My Drive/416 Pro/416 ORTAKLAR/giray_yeni_collab/\"\n"],"metadata":{"id":"mX5ZyN0DJz1j","ExecuteTime":{"end_time":"2023-06-17T14:40:00.239064Z","start_time":"2023-06-17T14:40:00.234759Z"}},"id":"mX5ZyN0DJz1j"},{"cell_type":"code","execution_count":null,"outputs":[],"source":["!pip3 install torch torchvision\n","\n","import torch\n","import torchvision\n","from torchvision import ops\n","from torchvision import transforms\n","from torchvision.transforms import ToTensor\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","print(torch.__version__)\n","print(torchvision.__version__)"],"metadata":{"id":"rvklVU-_Jz1k"},"id":"rvklVU-_Jz1k"},{"cell_type":"code","execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/fasterrcnn_mobilenet_v3_large_fpn-fb6a3cc7.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_mobilenet_v3_large_fpn-fb6a3cc7.pth\n","100%|██████████| 74.2M/74.2M [00:01<00:00, 73.2MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Total Backbone Network Layers:  100 \n","\n","New model head:  FastRCNNPredictor(\n","  (cls_score): Linear(in_features=1024, out_features=3, bias=True)\n","  (bbox_pred): Linear(in_features=1024, out_features=12, bias=True)\n",") \n","\n","Device:  cuda \n","\n","\n"]}],"source":["import os\n","import cv2\n","import numpy as np\n","from PIL import Image\n","import math\n","import time\n","from matplotlib import pyplot\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","\n","# We mainly trained on Google Collab with GPU, which sped up the training around 60 times\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, root, transforms):\n","        self.root = root\n","        self.transforms = transforms\n","        self.imgs = list(sorted([file for file in os.listdir(os.path.join(root, \"images\")) if \".DS_Store\" not in file]))\n","        self.labels = list(sorted([file for file in os.listdir(os.path.join(root, \"labels\")) if \".DS_Store\" not in file]))\n","\n","\n","    def __getitem__(self, idx):\n","        img_path = os.path.join(self.root, \"images\", self.imgs[idx])\n","        label_path = os.path.join(self.root, \"labels\", self.labels[idx])\n","        img = Image.open(img_path).convert(\"RGB\")\n","\n","        with open(label_path) as txt_file:\n","            lines = txt_file.readlines()\n","\n","        boxes = []\n","        labels = []\n","        for line in lines:\n","            class_label, x_center, y_center, width, height = map(float, line.strip().split())\n","            x_min = (x_center - width  / 2) * img.width\n","            y_min = (y_center - height / 2) * img.height\n","            x_max = (x_center + width  / 2) * img.width\n","            y_max = (y_center + height / 2) * img.height\n","\n","            boxes.append([x_min, y_min, x_max, y_max])\n","\n","            # dataset:    Emergency=0, Non-Emergency=1\n","            # model predicts: background=0, Emergency=1, Non-Emergency=2\n","            labels.append(int(class_label) + 1)\n","\n","        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n","        labels = torch.as_tensor(labels, dtype=torch.int64)\n","        image_id = torch.tensor([idx])\n","        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n","        iscrowd = torch.zeros((len(labels),), dtype=torch.int64)\n","\n","        target = {\"boxes\": boxes, \"labels\": labels, \"image_id\": image_id, \"area\": area, \"iscrowd\": iscrowd}\n","\n","        if self.transforms is not None:\n","            img = ToTensor()(img)\n","            img = self.transforms(img)\n","\n","        return img, target\n","\n","\n","    def __len__(self):\n","        return len(self.imgs)\n","\n","transform = transforms.Compose([\n","    transforms.ToPILImage(),\n","    transforms.Resize((224, 224)),\n","    transforms.RandomHorizontalFlip(),\n","    #transforms.ColorJitter(brightness=0.1, contrast=0.1),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","# Load the dataset\n","dataset_train = CustomDataset(root_folder + 'emergency_dataset/train', transforms=transform)\n","dataset_val = CustomDataset(root_folder +'emergency_dataset/valid', transforms=transform)\n","dataset_test = CustomDataset(root_folder + 'emergency_dataset/test', transforms=transform)\n","dataset_demo = CustomDataset(root_folder + 'emergency_dataset/demo', transforms=transform)\n","\n","# There can be multiple objects in each image\n","def my_collate(batch):\n","    data = [item[0] for item in batch]\n","    target = [item[1] for item in batch]\n","    return [data, target]\n","\n","\n","# memory efficient input processing\n","n = 1 if environment_ == \"collab\" else 0  # parallelization\n","batch_size_ = 50\n","train_dataloader = DataLoader(dataset_train, batch_size=batch_size_, shuffle=True, num_workers=n, collate_fn=my_collate)\n","val_dataloader = DataLoader(dataset_val, batch_size=batch_size_, shuffle=False, num_workers=n, collate_fn=my_collate)\n","test_dataloader = DataLoader(dataset_test, batch_size=batch_size_, shuffle=False, num_workers=n, collate_fn=my_collate)\n","demo_dataloader = DataLoader(dataset_demo, batch_size=batch_size_, shuffle=False, num_workers=n, collate_fn=my_collate)\n","\n","\n","# Plotting the results\n","training_loss_array = []\n","validation_loss_array = []\n","\n","\n","def draw_loss_plots(epochs):\n","    results_plot = pyplot.figure(figsize=(10, 8))\n","    pyplot.title(label=\"Training and Validation Loss vs Epochs\", loc=\"center\", y=1.0, fontsize=16, pad=35)\n","    pyplot.axis('off')\n","\n","    results_plot.add_subplot(1, 1, 1)\n","    pyplot.plot(training_loss_array, color=\"orange\")\n","    pyplot.plot(validation_loss_array, color=\"blue\")\n","    pyplot.ylabel('Loss', fontsize=16)\n","    pyplot.xlabel('Epochs', fontsize=16)\n","    pyplot.legend(['Training Loss', 'Validation Loss'], fontsize=16)\n","    pyplot.xticks(np.arange(1, epochs , 5))\n","\n","    results_plot.subplots_adjust(top=0.75)\n","    pyplot.tight_layout()\n","    pyplot.savefig(root_folder + \"loss_plot.jpg\")\n","    pyplot.show()\n","\n","\n","\n","# Load a pre-trained Faster R-CNN model\n","model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(weights='DEFAULT', trainable_backbone_layers=0)\n","\n","num_classes = 3  # Background, Non-Emergency Vehicle, Emergency Vehicle\n","\n","# Freeze all layers\n","print(\"Total Backbone Network Layers: \", len(list(model.parameters())), \"\\n\")\n","for index, param in enumerate(model.parameters()):\n","    param.requires_grad = False\n","\n","# Replace the pre-trained head with a new one four our dataset\n","number_of_classifier_input_features = model.roi_heads.box_predictor.cls_score.in_features\n","model.roi_heads.box_predictor = FastRCNNPredictor(number_of_classifier_input_features, num_classes)\n","print(\"New model head: \", model.roi_heads.box_predictor, \"\\n\")\n","\n","# Move the model to the available device.\n","# We mainly trained on Google Collab with GPU, which sped up the training around 120 times\n","print(\"Device: \", device, \"\\n\")\n","model.to(device)\n","print()"],"metadata":{"id":"ep4y5oH1Jz1l","ExecuteTime":{"end_time":"2023-06-17T14:41:05.394826Z","start_time":"2023-06-17T14:41:05.020764Z"},"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687167255123,"user_tz":-180,"elapsed":35399,"user":{"displayName":"M Giray Nacakcı","userId":"07330273579537138423"}},"outputId":"50350039-8b0b-4e77-8d41-d11c86a60457"},"id":"ep4y5oH1Jz1l"},{"cell_type":"markdown","source":["# TRAINING"],"metadata":{"collapsed":false,"id":"H9BeHysqJz1n"},"id":"H9BeHysqJz1n"},{"cell_type":"code","execution_count":null,"outputs":[],"source":["\n","# Construct an optimizer\n","params = [p for p in model.parameters() if p.requires_grad]\n","\n","# optimizer = torch.optim.SGD(params, lr=0.01, momentum=0.9, weight_decay=0.0005)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","# Learning rate scheduler which decreases the learning rate by 10x every 3 epochs\n","lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n","\n","training_loss_array.clear()\n","validation_loss_array.clear()\n","\n","print(\"- - - - -   TRAINING STARTED   - - - - - - \")\n","\n","num_epochs = 20\n","min_val_loss = math.inf\n","training_start = time.time()\n","\n","model.train()\n","for epoch in range(num_epochs):\n","    train_loss = 0\n","    train_epoch_time = time.time()\n","\n","    batch_counter = 0\n","    for images, targets in train_dataloader:  # for each Batch\n","\n","        batch_counter += 1\n","        batch_start_time = time.time()\n","\n","        images = list(image.to(device) for image in images)\n","        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n","\n","        loss_dict = model(images, targets)\n","        losses = sum(loss for loss in loss_dict.values())\n","        train_loss += losses.item()\n","\n","        optimizer.zero_grad()\n","        losses.backward()\n","        optimizer.step()\n","\n","    train_loss = round(train_loss / len(train_dataloader), 3)\n","    print(f\"\\nEpoch #{epoch+1}  Train Loss: {train_loss}  \", end=\"  \")\n","    print(\"  calculated in \", str(int(time.time() - train_epoch_time)), \"seconds. \\n \")\n","    training_loss_array.append(train_loss)\n","\n","\n","    \"\"\" VALIDATION \"\"\"\n","\n","    \"\"\"\n","    Problem was: model(images, targets) returns losses while in model.train() mode.\n","    But returns prediction results in model.eval() mode. It is designed by pytorch that way.\n","\n","    Solution is: removed model.eval() from Validation part.\n","    It is not recommended to run evaluation on train mode, but since all backbone layers are frozen, it is okay.\n","    \"\"\"\n","    val_loss = 0\n","    start_val_time = time.time()\n","    with torch.no_grad():\n","        for images, targets in val_dataloader:  # processed in batches\n","\n","            images = list(image.to(device) for image in images)\n","            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n","\n","            loss_dict = model(images, targets)\n","\n","            losses = sum(loss for loss in loss_dict.values())\n","            val_loss += losses.item()\n","\n","    val_loss = round(val_loss / len(val_dataloader), 3)\n","    print(f\"Epoch #{epoch+1} Validation Loss: {val_loss}\", end=\"    \")\n","    print(\"calculated in \", str(int(time.time() - start_val_time)), \"seconds.\")\n","    validation_loss_array.append(val_loss)\n","\n","    # save model if validation loss has decreased\n","    if val_loss < min_val_loss:\n","        print(f\"Validation Loss Decreased({min_val_loss:.3f}--->{val_loss:.3f}) \\t Saving The Model\\n\")\n","        torch.save(model.state_dict(), root_folder + 'saved_model.pth')\n","        min_val_loss = val_loss\n","    else:\n","        print()\n","\n","    # Update the learning rate\n","    lr_scheduler.step()\n","\n","\n","print(\"\\n\\n* * * * TRAINING  COMPLETED IN \", str(int(time.time() - training_start)), \"seconds. * * * * \\n \")\n","\n","draw_loss_plots(num_epochs)\n"],"metadata":{"id":"1cxS3f6FJz1o","ExecuteTime":{"end_time":"2023-06-17T14:33:36.899409Z","start_time":"2023-06-17T14:33:23.968747Z"}},"id":"1cxS3f6FJz1o"},{"cell_type":"markdown","source":["# TESTING (Saved Model)\n","\n","<br>\n","\n","### (Currently: CANNOT Run in Local, if model was trained in cloud (GOOGLE COLLAB)).\n","\n","<br>\n","\n","## First, Run the \"INITIALIZATION\" section. But No need to run Training if saved_model.pth exists."],"metadata":{"collapsed":false,"id":"lCxuuBjSJz1p"},"id":"lCxuuBjSJz1p"},{"cell_type":"code","execution_count":8,"outputs":[],"source":["\n","def test_evaluation(actual_classes_and_bboxes, predicted_classes_and_bboxes):\n","\n","    true_positives_for_vehicle = 0\n","    false_positives = 0\n","    false_negatives = 0\n","    accurate_emergency_class = 0\n","    wrong_emergency_class = 0\n","    total_actual_vehicle = 0\n","    total_predicted_vehicle = 0\n","    actual_labels_list = []\n","    predicted_labels_list = []\n","\n","\n","    for actual_for_image, predicted_for_image in zip(actual_classes_and_bboxes, predicted_classes_and_bboxes):\n","\n","        actual_boxes = actual_for_image['boxes']\n","        actual_labels = actual_for_image['labels']\n","\n","        predicted_boxes = predicted_for_image['boxes']\n","        predicted_labels = predicted_for_image['labels']\n","        scores = predicted_for_image['scores']\n","\n","        # retain only the highest score ones of overlapping detections\n","        non_maximum_supressed = torchvision.ops.batched_nms(predicted_boxes, scores, predicted_labels, 0.5)\n","        predicted_boxes, scores, predicted_labels = (predicted_boxes[non_maximum_supressed], scores[non_maximum_supressed], predicted_labels[non_maximum_supressed])\n","\n","        # remove predicted bounding boxes which has low score\n","        lowest_removed = torch.where(scores > 0.1)[0]\n","        predicted_boxes, scores, predicted_labels = (predicted_boxes[lowest_removed], scores[lowest_removed], predicted_labels[lowest_removed])\n","\n","        # Comparison of predicted and actual\n","        total_actual_vehicle += len(actual_boxes)\n","        total_predicted_vehicle += len(predicted_boxes)\n","\n","        intersection_over_union = (ops.box_iou(actual_boxes, predicted_boxes)).cpu().numpy()\n","\n","        if len(intersection_over_union[0]) == 0: # no box is predicted for this image\n","            false_negatives += len(actual_boxes)  # actually exists but not predicted\n","        else:\n","            # intersection over union above is a 2d array (table) for IoU values\n","            # which holds IoU values for every combination of actual vs. predicted box pair\n","\n","            predicted_boxes_if_assigned = [False] * len(predicted_labels)\n","            for a, row in enumerate(intersection_over_union):  # for each ground_truth box\n","                max_iou_of_predicted_boxes = np.max(row)\n","                if max_iou_of_predicted_boxes < 0.5:\n","                    false_negatives += 1\n","                else:\n","\n","                    # prioritize assigning highest IoU first\n","                    for p, cell in sorted(enumerate(row), key=lambda x:x[1], reverse=True):\n","\n","                        if cell >= 0.5:\n","                            if predicted_boxes_if_assigned[p] == False:  # prevent assigning same predicted box to multiple ground_truth boxes\n","\n","                                true_positives_for_vehicle += 1\n","                                predicted_boxes_if_assigned[p] = True\n","\n","                                actual_label_ = int(actual_labels[a])\n","                                predicted_label_ = int(predicted_labels[p])\n","                                if actual_label_ == predicted_label_:\n","                                    accurate_emergency_class += 1\n","                                else:\n","                                    wrong_emergency_class += 1\n","                                actual_labels_list.append(actual_label_)\n","                                predicted_labels_list.append(predicted_label_)\n","\n","            for is_assigned in predicted_boxes_if_assigned:\n","                if is_assigned == False:\n","                    false_positives += 1\n","\n","    return true_positives_for_vehicle, false_positives, false_negatives, accurate_emergency_class, wrong_emergency_class, total_actual_vehicle, total_predicted_vehicle, actual_labels_list, predicted_labels_list\n","\n","\n","\n","def print_test_results(tp_for_vehicle, fp, fn, accurate_emergency_class, wrong_emergency_class, actual_vehicle, predicted_vehicle):\n","\n","    print(\"\\nActual Vehicle : \", actual_vehicle, \" Predicted Vehicle : \", predicted_vehicle)\n","\n","    precision = 0\n","    if tp_for_vehicle + fp != 0:\n","        precision = round(tp_for_vehicle / (tp_for_vehicle + fp), 2)\n","    recall = 0\n","    if tp_for_vehicle + fn != 0 :\n","        recall = round(tp_for_vehicle / (tp_for_vehicle + fn), 2)\n","    accuracy = 0\n","    if accurate_emergency_class + wrong_emergency_class != 0:\n","        accuracy = round(accurate_emergency_class / (accurate_emergency_class + wrong_emergency_class), 2)\n","\n","    print(\"Bounding box Precision: \", precision, \"  Recall: \", recall, \" ;  classification ACCURACY:  \", accuracy, \"\\n\\n\")\n","\n","\n","def classification_confusion_matrix(actual, predicted):\n","    confusion_matrix_ = confusion_matrix(actual, predicted)\n","    cm_display = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix_, display_labels=[\"Emergency\", \"Non-Emergency\"])  # \"background\",\n","    cm_display.plot()\n","    pyplot.title('Confusion Matrix of Test Set (TruePos. b.boxes)', x=0.2, fontsize=17)\n","    pyplot.ylabel('Actual Class', fontsize=13)\n","    pyplot.xlabel('Predicted Class', fontsize=13)\n","    pyplot.xticks(rotation=90)\n","    pyplot.tight_layout()\n","\n","    pyplot.savefig(root_folder + \"confusion_matrix.jpg\")\n","    pyplot.show()\n","\n"],"metadata":{"id":"JKU_0eNm4fpT","executionInfo":{"status":"ok","timestamp":1687167998389,"user_tz":-180,"elapsed":559,"user":{"displayName":"M Giray Nacakcı","userId":"07330273579537138423"}}},"id":"JKU_0eNm4fpT"},{"cell_type":"code","execution_count":null,"outputs":[],"source":["\"\"\" TESTING \"\"\"\n","\n","start_test_time = time.time()\n","\n","# attempt to run cloud trained model on local. does not work.\n","#if environment_== \"local\":\n","#    model = torch.load(model, map_location=torch.device('cpu'))\n","#    model.to(device)\n","\n","# Load the best model\n","model.load_state_dict(torch.load(root_folder +'model_6.pth'))\n","\n","print(\"- - - - -   TESTING RESULTS:   - - - - - - \")\n","\n","true_positives_for_vehicle =  false_positives = false_negatives = accurate_emergency_class = 0\n","wrong_emergency_class = total_actual_vehicle = total_predicted_vehicle = 0\n","actual_labels_list = []\n","predicted_labels_list = []\n","\n","model.eval()\n","with torch.no_grad():\n","\n","    for images, targets in test_dataloader:  # processed in Batches\n","        images = list(image.to(device) for image in images)\n","        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n","        results = model(images, targets)\n","\n","        true_positives_for_vehicle_, false_positives_, false_negatives_, accurate_emergency_class_, wrong_emergency_class_, total_actual_vehicle_, total_predicted_vehicle_, actual_labels_list_, predicted_labels_list_ = test_evaluation(targets, results)\n","        true_positives_for_vehicle += true_positives_for_vehicle_\n","        false_positives += false_positives_\n","        false_negatives += false_negatives_\n","        accurate_emergency_class += accurate_emergency_class_\n","        wrong_emergency_class += wrong_emergency_class_\n","        total_actual_vehicle += total_actual_vehicle_\n","        total_predicted_vehicle += total_predicted_vehicle_\n","        actual_labels_list.extend(actual_labels_list_)\n","        predicted_labels_list.extend(predicted_labels_list_)\n","\n","\n","print(\"\\n\\n* * * * TEST RESULTS Calculated in \", str(int(time.time() - start_test_time)), \"seconds * * * *\")\n","\n","print_test_results(true_positives_for_vehicle, false_positives, false_negatives, accurate_emergency_class, wrong_emergency_class, total_actual_vehicle, total_predicted_vehicle)\n","\n","classification_confusion_matrix(actual_labels_list, predicted_labels_list)\n"],"metadata":{"id":"gY2Ohd8pJz1q"},"id":"gY2Ohd8pJz1q"},{"cell_type":"markdown","source":["# VISUAL DEMO\n","of Detection on Selected Images\n","\n","<br>\n","\n","## First, Run the \"INITIALIZATION\" section."],"metadata":{"id":"kDkNlAZZthWY"},"id":"kDkNlAZZthWY"},{"cell_type":"code","source":["model_no = 6\n","\n","# 225 test images\n","#test_images_paths = list(sorted([str(root_folder + 'emergency_dataset/test/images/') + str(file) for file in os.listdir(root_folder + 'emergency_dataset/test/images') if \".DS_Store\" not in file]))\n","demo_images_paths = list(sorted([str(root_folder + 'emergency_dataset/demo/images/') + str(file) for file in os.listdir(root_folder + 'emergency_dataset/demo/images') if \".DS_Store\" not in file]))\n","\n","# Load the best model\n","model.load_state_dict(torch.load(root_folder +'model_' + str(model_no) + '.pth'))\n","model.eval()\n","with torch.no_grad():\n","\n","    iteration = 0\n","\n","    for tensor_images, targets in demo_dataloader:\n","\n","        tensor_images = list(image.to(device) for image in tensor_images)\n","        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n","        results = model(tensor_images, targets)\n","\n","        for i, (actual_for_image, predicted_for_image) in enumerate(zip(targets, results)):  # each image\n","            actual_boxes = actual_for_image['boxes']\n","            actual_labels = actual_for_image['labels']\n","            predicted_boxes = predicted_for_image['boxes']\n","            predicted_labels = predicted_for_image['labels']\n","            scores = predicted_for_image['scores']\n","\n","            # retain only the highest score ones of overlapping detections\n","            non_maximum_supressed = torchvision.ops.batched_nms(predicted_boxes, scores, predicted_labels, 0.5)\n","            predicted_boxes, scores, predicted_labels = (predicted_boxes[non_maximum_supressed], scores[non_maximum_supressed], predicted_labels[non_maximum_supressed])\n","\n","            # remove predicted bounding boxes which has low score\n","            lowest_removed = torch.where(scores > 0.1)[0]\n","            predicted_boxes, scores, predicted_labels = (predicted_boxes[lowest_removed], scores[lowest_removed], predicted_labels[lowest_removed])\n","\n","            # annotate images with boxes\n","            actual_annotated_img = cv2.imread(demo_images_paths[batch_size_*iteration + i])\n","            predictions_annotated_img = cv2.imread(demo_images_paths[batch_size_*iteration + i])\n","\n","            for e, box in enumerate(actual_boxes):\n","                color_ = (0,0,0)\n","                label_ = actual_labels[e]\n","                if int(label_) == 1:\n","                    color_ = (0,255,0)\n","                elif int(label_) == 2:\n","                    color_ = (0,255,255)\n","                cv2.rectangle(actual_annotated_img, (int(box[0]),int(box[1])), (int(box[2]),int(box[3])), color=color_, thickness=2, lineType=cv2.LINE_AA)\n","\n","            for e, box in enumerate(predicted_boxes):\n","                color_ = (0,0,0)\n","                label_ = predicted_labels[e]\n","                if int(label_) == 1:\n","                    color_ = (0,255,0)\n","                elif int(label_) == 2:\n","                    color_ = (0,255,255)\n","                cv2.rectangle(predictions_annotated_img, (int(box[0]),int(box[1])), (int(box[2]),int(box[3])), color=color_, thickness=2, lineType=cv2.LINE_AA)\n","                cv2.putText(predictions_annotated_img, text=str(round(float(scores[e]), 2)), org=(int(box[0]), int(box[1]-5)), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.6, color=color_, thickness=2, lineType=cv2.LINE_AA)\n","\n","            # concatenate and display side by side: image with Actual annotations, white space, image with Predicted annotations\n","            white_space = np.full((224, 50, 3), 255, dtype=np.uint8)\n","            annotated_image = np.concatenate((actual_annotated_img, white_space, predictions_annotated_img), axis=1, dtype=np.uint8)\n","\n","            # pyplot.figure(figsize=(10, 8))\n","            pyplot.imshow(cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB))\n","            pyplot.title(label=\"Actual vs Predicted\", loc=\"center\", y=1.0, fontsize=16, pad=35)\n","            #results_plot.subplots_adjust(top=0.75)\n","            #pyplot.axis('off')\n","            #pyplot.text(.5, .9, \"Green = Emergency , Yellow = Non-Emergency\", ha='center')\n","            pyplot.tick_params(axis='x', which='both', bottom=False)\n","            pyplot.tick_params(axis='y', which='both', left=False)\n","            pyplot.xticks([])\n","            pyplot.yticks([])\n","            pyplot.xlabel('Green = Emergency , Yellow = Non-Emergency', fontsize=13)\n","            pyplot.tight_layout()\n","            img_name = demo_images_paths[batch_size_*iteration + i].split(\"/\")[-1].split(\"_\")[0]\n","            pyplot.savefig(root_folder + \"demo_images/\" + str(img_name) + \"_m\" + str(model_no) + \".jpg\", dpi=450)\n","            pyplot.clf()  # clear memory\n","\n","            #cv2.imwrite(root_folder + \"demo_images/\" + str(img_name) + \"_m\" + str(model_no) + \".jpg\", annotated_image)\n","\n","        iteration += 1\n","\n","print(\"done\")\n"],"metadata":{"id":"Rjcu8-mGtz07"},"id":"Rjcu8-mGtz07","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}